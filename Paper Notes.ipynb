{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Notes for Transfer Learning\n",
    "\n",
    "### 2019-08-13\n",
    "1. [A Hybrid Instance-based Transfer Learning Method (Weighted, NIPS 2018, workshop)](https://arxiv.org/pdf/1812.01063.pdf)\n",
    "2. [Structural Domain Adaptation With Latent Graph Alignment (2018, ICIP)](https://ieeexplore.ieee.org/abstract/document/8451245)\n",
    "\n",
    "### 2019-08-14\n",
    "3. [Domain Adaptation via Transfer Component Analysis (IJCAI-09)](https://www.cse.ust.hk/~qyang/Docs/2009/TCA.pdf) <font color=\"#dd0000\">TCA</font><br />\n",
    "4. [Transfer Learning via Dimensionality Reduction(AAAI-08)](https://www.aaai.org/Papers/AAAI/2008/AAAI08-108.pdf)\n",
    "5. [Transfer feature learning with joint distribution adaptation(ICCV-13)](http://ise.thss.tsinghua.edu.cn/~mlong/doc/joint-distribution-adaptation-iccv13.pdf) <font color=\"#dd0000\">JDA</font><br />\n",
    "\n",
    "### 2019-08-15\n",
    "6. [A Comparative Study on Unsupervised Domain Adaptation Approaches for Coffee Crop Mapping](https://arxiv.org/pdf/1806.02400.pdf)\n",
    "\n",
    "### 2019-08-16\n",
    "7. [Return of frustratingly easy domain adaptation(AAAI-16)](https://arxiv.org/pdf/1511.05547.pdf) <font color=\"#dd0000\">CORAL</font><br />\n",
    "8. [Transfer Joint Matching for Unsupervised Domain Adaptation(CVPR-14)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.703.3481&rep=rep1&type=pdf) <font color=\"#dd0000\">TJM</font><br />\n",
    "\n",
    "### 2019-08-18\n",
    "9. [Unsupervised Visual Domain Adaptation Using Subspace Alignment(ICCV-13)](https://hal.archives-ouvertes.fr/hal-00869417/document) <font color=\"#dd0000\">SA</font><br />\n",
    "10. [Joint Geometrical and Statistical Alignment for Visual Domain Adaptation(CVPR-17)](https://arxiv.org/pdf/1705.05498.pdf) <font color=\"#dd0000\">JGSA</font><br />\n",
    "\n",
    "### 2019-08-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-08-13\n",
    "\n",
    "\n",
    "## Paper 1\n",
    "### A Hybrid Instance-based Transfer Learning Method \n",
    "\n",
    "基于实例的混合权重迁移学习\n",
    "\n",
    "Journal: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018 (NIPS workshop)\n",
    "\n",
    "Bibcitation:\n",
    "\n",
    "@article{antropova2018machine,\n",
    "  title={Machine Learning for Health (ML4H) Workshop at NeurIPS 2018},\n",
    "  author={Antropova, Natalia and Bream, Andrew and Beaulieu-Jones, Brett K and Chen, Irene and Chivers, Corey and Dalca, Adrian and Finlayson, Sam and Fiterau, Madalina and Fries, Jason Alan and Ghassemi, Marzyeh and others},\n",
    "  journal={arXiv preprint arXiv:1811.07216},\n",
    "  year={2018}\n",
    "}\n",
    "\n",
    "#### 内容\n",
    "\n",
    "本文是基于实例(instance based)的迁移学习典例，应用于医疗的人脸识别与受伤预测当中，为一种**监督学习**，其来源于以下均值误差的表达，讲来自目标域的实例的误差值表示为来自目标域与源域的两部分：\n",
    "\n",
    "\\begin{aligned} \\mathbb{E}_{x \\sim P_{T}}[\\underbrace{\\mathcal{L}(\\mathcal{A}(x), y)}_{\\epsilon(x)}] &=\\int \\epsilon(x) P_{T}(x) d x=\\int \\epsilon(x) \\underbrace{\\left[\\alpha+(1-\\alpha) \\frac{P_{S}(x)}{P_{S}(x)}\\right]}_{=1} P_{T}(x) d x \\\\ &=\\alpha \\mathbb{E}_{x \\sim P_{T}}[\\epsilon(x)]+(1-\\alpha) \\mathbb{E}_{x \\sim P_{S}}\\left[\\epsilon(x) \\frac{P_{T}(x)}{P_{S}(x)}\\right] \n",
    "\\end{aligned}\n",
    "\n",
    "上式子第一项为target importance, 第二项为source importance， 由于在应用中源域与目标域的实例均为有限值，目标函数可写成：\n",
    "\n",
    "$$\\Theta^{*}=\\underset{\\Theta}{\\arg \\min }\\left(\\frac{\\alpha}{N_{T}} \\sum_{i=1}^{N_{T}} \\epsilon\\left(x_{i}, \\Theta\\right)+\\frac{1-\\alpha}{N_{S}} \\sum_{j=1}^{N_{S}} \\epsilon\\left(x_{j}, \\Theta\\right) w_{x_{j}}\\right)$$\n",
    "\n",
    "作者的创新点在于之前的文献在考虑$w_{x_j}$时**只估算数据分布之前的权重**，而不考虑任务带来的影响，所以作者将权重分为域与任务两部分：\n",
    "$$w_x = w_{domain,x} + w_{task,x}$$\n",
    "\n",
    "其中$w_{domain,x}$是通过构造分类器来鉴别实例是来自于源域或者目标域获得,即（$P_T(x)/P_S(x)$）。对于$w_{task,x}$, 作者使用了不确定性（uncertainty）来作为权重，比如，在二分类中，此不确定性是实例$x$对于任务预测边界(decision boundary)的误差，若预测准确，则此权重为正，若预测错误，则此权重为负。\n",
    "\n",
    "所以这个源域权重分为两部分，第一个为域权重，**表明若源域的数据越像目标域，则权重越大，反之。** 第二个为任务权重， **表明若源域的数据对于目标预测准确，则权重为正，并且越准确此权重越大，反之则为负，越不准确此权重越小。**对于权重的构造，相当于筛选了源域越像目标域的实例，并且筛选了对预测有帮助的实例。在直觉上来说也是非常有效的。\n",
    "\n",
    "并且通过实验证明这种方法在人脸识别与伤情预测中均取得了很好的效果。\n",
    "\n",
    "## Paper 2\n",
    "### Structural Domain Adaptation With Latent Graph Alignment\n",
    "Conference: 2018 25th IEEE International Conference on Image Processing (ICIP)\n",
    "\n",
    "Bibcitation:\n",
    "@inproceedings{zhang2018structural,\n",
    "  title={Structural Domain Adaptation with Latent Graph Alignment},\n",
    "  author={Zhang, Yue and Miao, Shun and Liao, Rui},\n",
    "  booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},\n",
    "  pages={3753--3757},\n",
    "  year={2018},\n",
    "  organization={IEEE}\n",
    "}\n",
    "\n",
    "#### 内容\n",
    "\n",
    "本文针对最小化**最大均值差异(Maximum Mean Discrepancy)** 提出了相对应的领域自适应方法，此种方法在TCA,JDA等方法的启发下，在RKHS中最小化MMD的同时通过构造Laplacian Graph进行图谱分析来最大化结构的相似度。文章分为两个步骤，一为通过寻找一对线性权重(源域权重与目标域权重)$W_s$与$W_t$来最小化**经验式**的MMD，具体形式如下：\n",
    "\n",
    "$$\\operatorname{MMD}\\left(W_{s}, W_{t}\\right)=\\left\\|\\frac{1}{n_{s}} \\sum_{i=1}^{n_{s}} W_{s}^{T} x_{i}-\\frac{1}{n_{t}} \\sum_{j=1}^{n_{t}} W_{t}^{T} \\hat{x}_{j}\\right\\|$$\n",
    "\n",
    "其中$W_s$与$W_t$为我们需要求得的映射，$n_s$为源域实例数目，$n_t$为目标域实例数目，$x_i$与$\\hat{x}_j$分别为源域与目标域的实例。 同时在求得$W_s$与$W_t$后，通过构造拉普拉斯矩阵来衡量两个域(或可看成流行)的结构相似度。其损失函数可定义为：\n",
    "\n",
    "$$\\mathcal{L}\\left(W_{s}, W_{t}\\right)=\\left\\|\\Lambda\\left(U^{T} L_{s} U\\right)-\\Lambda\\left(L_{t}\\right)\\right\\|_{2}^{2}$$\n",
    "\n",
    "其中$L_s$与$L_t$为源域与目标域正则化的拉普拉斯矩阵。所以迁移的问题便转化为在使得这个函数尽量小的情况下去最小化MMD。如下式所示：\n",
    "\\begin{aligned} \\min _{\\left\\{W_{s}, W_{t}\\right\\} \\in \\Omega} &\\left\\|\\frac{1}{n_{s}} \\sum_{i} W_{s}^{T} x_{i}-\\frac{1}{n_{t}} \\sum_{j} W_{t}^{T} \\hat{x}_{j}\\right\\|^{2} \\\\ & \\text { s.t. }\\left\\|\\Lambda\\left(U^{T} L_{s} U\\right)-\\Lambda\\left(L_{t}\\right)\\right\\|_{2}^{2} \\leq \\epsilon \\end{aligned}\n",
    "\n",
    "然而对于求得最优$W_s$与$W_t$，这不是一个凸问题，作者采用迭代方法将问题分解为两个步骤：\n",
    "\n",
    "- ###  1 MMD-Stage\n",
    "在第一个步骤当中，问题转化为：在第$i$次迭代中，给定$W_t^i$，可解出$W_s^i$:\n",
    "\n",
    "$$W_{s}^{i}=\\underset{W}{\\operatorname{argmin}} \\operatorname{MMD}\\left(W, W_{t}\\right), \\quad \\text { s.t. } W_{t}=W_{t}^{i}$$\n",
    "\n",
    "这是一个标准的MMD模型，存在闭合解，可参考TCA等文章求得近似解。\n",
    "\n",
    "- ###  2 LGA-Stage\n",
    "在第二个步骤当中，问题转化为：在第$i$次迭代中，给定$W_s^i$，可解出$W_{t+1}^i$:\n",
    "\n",
    "$$ W_{t}^{i+1}=\\underset{W}{\\operatorname{argmin}} \\mathcal{L}\\left(W_{s}^{i}, W\\right)$$\n",
    "\n",
    "为解决谱优化不存在闭合解的问题，作者通过构造中间图后通过sgd(梯度下降)的方法来迭代求得$W_{t+1}$。其中拉普拉斯的图构造通过热核(heat kernel)权重进行构造。\n",
    "\n",
    "其余具体的constraints与估计推导与concern可参考原文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-08-14\n",
    "\n",
    "## Paper 1\n",
    "\n",
    "### Domain Adaptation via Transfer Component Analysis(1) & Transfer Learning via Dimensionality Reduction(2)\n",
    "\n",
    "\n",
    "Conference 1: Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence (IJCAI-09)\n",
    "\n",
    "Conference 2: Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (2008) (AAAI-08)\n",
    "\n",
    "Bibcitation1: @article{pan2010domain,\n",
    "  title={Domain adaptation via transfer component analysis},\n",
    "  author={Pan, Sinno Jialin and Tsang, Ivor W and Kwok, James T and Yang, Qiang},\n",
    "  journal={IEEE Transactions on Neural Networks},\n",
    "  volume={22},\n",
    "  number={2},\n",
    "  pages={199--210},\n",
    "  year={2010},\n",
    "  publisher={IEEE}\n",
    "}\n",
    "\n",
    "Bibcitation2: @inproceedings{pan2008transfer,\n",
    "  title={Transfer learning via dimensionality reduction.},\n",
    "  author={Pan, Sinno Jialin and Kwok, James T and Yang, Qiang and others},\n",
    "  booktitle={AAAI},\n",
    "  volume={8},\n",
    "  pages={677--682},\n",
    "  year={2008}\n",
    "}\n",
    "\n",
    "#### 内容\n",
    "\n",
    "今天介绍的是两篇在迁移学习当中非常重要以及非常优秀的成果，TCA与JDA。首先TCA是一种unsupervised learning，相关的两篇论文的主要思想是**通过学习一个高维映射(kernel)，在此空间中源域与目标域的概率分布与标签的条件边缘保持近似（$p(X_S) \\approx p(X_T)$，$p(y|X_S) \\approx p(y|X_T)$**，并且在此条件下去最大化共同潜在域中的方差，以维持原有数据的特征属性（此限制具体参考\n",
    "[MVU](http://new.aaai.org/Papers/AAAI/2006/AAAI06-280.pdf)）。具体的步骤如下：\n",
    "\n",
    "- 1 首先在RKHS中定义如下距离(Maximum Mean Discrepancy)：\n",
    "$$\\operatorname{Dist}(\\mathrm{X}, \\mathrm{Y})=\\left\\|\\frac{1}{n_{1}} \\sum_{i=1}^{n_{1}} \\phi\\left(x_{i}\\right)-\\frac{1}{n_{2}} \\sum_{i=1}^{n_{2}} \\phi\\left(y_{i}\\right)\\right\\|_{\\mathcal{H}}$$\n",
    "将上式替换成源域与目标域之间的关系，可得：\n",
    "$$\\operatorname{Dist}\\left(X_{S}^{\\prime}, X_{T}^{\\prime}\\right)=\\left\\|\\frac{1}{n_{1}} \\sum_{i=1}^{n_{1}} \\phi\\left(x_{S_{i}}\\right)-\\frac{1}{n_{2}} \\sum_{i=1}^{n_{2}} \\phi\\left(x_{T_{i}}\\right)\\right\\|_{\\mathcal{H}}^{2}$$\n",
    "\n",
    "- 2 目标上式最小化，构造核矩阵$K$与$L$，如下图所示，图中有一些typo，比如列核矩阵应该有转置符号，不然维度会出现不对称：\n",
    "![](https://cdn.mathpix.com/snip/images/f4ewSgOVoLXfm9cdN5HP31bCvxucTpcU694CkBFwqqQ.original.fullsize.png)（参考:[MMD推导](https://zhuanlan.zhihu.com/p/63026435)）\n",
    "在原文中，$K=\\left[\\begin{array}{ll}{K_{S, S}} & {K_{S, T}} \\\\ {K_{T, S}} & {K_{T, T}}\\end{array}\\right]$, \n",
    "$L_{i j}=\\left\\{\\begin{array}{ll}{\\frac{1}{n_{1}^{2}}} & {x_{i}, x_{j} \\in X_{s r c}} \\\\ {\\frac{1}{n_{2}^{2}}} & {x_{i}, x_{j} \\in X_{t a r}} \\\\ {-\\frac{1}{n_{1} n_{2}}} & {\\text { otherwise }}\\end{array}\\right.$\n",
    "\n",
    "- 3 有了目标所需要的迹优化，根据MVU的启发，为保持数据散度/方差（距离）的一致性，需要最大化$trace(K)$，即最小化$-trace(K)$，再添加一个超参数，整个优化问题如下：\n",
    "\\begin{array}{cl}{\\min _{K=\\tilde{K}+\\varepsilon I}} & {\\operatorname{trace}(K L)-\\lambda \\operatorname{trace}(K)} \\\\ {\\text { s.t. }} & {K_{i i}+K_{j j}-2 K_{i j}=d_{i j}^{2}, \\forall(i, j) \\in \\mathcal{N}} \\\\ {} & {K \\mathbf{1}=\\mathbf{0}, \\widetilde{K} \\succeq 0}\\end{array}\n",
    "\n",
    "- 4 以上的问题可写为半定规划（SDP）来求得最优$K$，在有了$K$之后，可对$K$使用PCA来降维至$m\\times(n_1+n_2)$维，对应的分别前$n_1$列即为源数据，后$n_2$列即为目标数据。然而此种方法无法泛化，不可对新的未知数据进行分类，所以作者在现有的核函数上通过降维构造一个$W$矩阵，来构造理想中的核矩阵（根据empirical kernel map）：\n",
    "$$\\widetilde{K}=\\left(K K^{-1 / 2} \\widetilde{W}\\right)\\left(\\widetilde{W}^{\\top} K^{-1 / 2} K\\right)=K W W^{\\top} K$$\n",
    "其中$W=K^{-1 / 2} \\widetilde{W} \\in \\mathbb{R}\\left(n_{1}+n_{2}\\right) \\times m$为所需要求得映射矩阵。那么当有新的观测值出现时，可通过$\\widetilde{k}\\left(x_{i}, x_{j}\\right)=k_{x_{i}}^{\\top} W W^{\\top} k_{x_{j}}$去映射至核矩阵。\n",
    "\n",
    "- 5 最终问题变为以下的问题：\n",
    "\\begin{array}{cl}{\\min _{W}} & {\\operatorname{tr}\\left(W^{\\top} W\\right)+\\mu \\operatorname{tr}\\left(W^{\\top} K L K W\\right)} \\\\ {\\text { s.t. }} & {W^{\\top} K H K W=I}\\end{array}\n",
    "第一项正则是为了防止出现零解与奇异解，第二项则是维持降维变换$W^TK$的散度。确保数据再映射后结构不变。上式可通过构造拉格朗日算子并求导得到以下的优化问题：\n",
    "\\begin{array}{c}{\\min _{W} \\operatorname{tr}\\left(\\left(W^{\\top} K H K W\\right)^{\\dagger} W^{\\uparrow}(I+\\mu K L K) W\\right)} \\\\ \n",
    "or \\quad {\\max _{W} \\operatorname{tr}\\left(\\left(W^{\\top}(I+\\mu K L K) W\\right)^{-1} W^{\\top} K H K W\\right)}\\end{array}\n",
    "那么问题就转换为**广义瑞丽商**的求解。$W$可通过求得$(I+\\mu K L K)^{-1} K H K$的前m个特征矢量拼合而成。这就完成了类似在4步骤中的PCA降维。在$W^TK$的空间中使用kNN分类器即可进行标准模式下的机器学习。\n",
    "\n",
    "总结来说，TCA首先将原始数据映射至RKHS来构造一个较为高维($n_1+n_2 \\times n_1+n_2$)的核函数矩阵（以数据之间的内积形式的表示，这里我感觉有一些奇怪，可能是因为这个内积把target与source给关联起来，以达到最小距离的目的），然后再通过构造一个低维度的矩阵，将这个已经关联的数据降维至m，并保持数据之间的方差（结构），以此来得到较好的效果。与8-13 Paper2不同的是，这篇文章在降维时同时考虑到了矩阵结构的保持。\n",
    "\n",
    "\n",
    "## Paper 2\n",
    "\n",
    "### Transfer feature learning with joint distribution adaptation\n",
    "\n",
    "Conference: Proceedings of the IEEE international conference on computer vision (ICCV-2013)\n",
    "\n",
    "Bibcitation:@inproceedings{long2013transfer,\n",
    "  title={Transfer feature learning with joint distribution adaptation},\n",
    "  author={Long, Mingsheng and Wang, Jianmin and Ding, Guiguang and Sun, Jiaguang and Yu, Philip S},\n",
    "  booktitle={Proceedings of the IEEE international conference on computer vision},\n",
    "  pages={2200--2207},\n",
    "  year={2013}\n",
    "}\n",
    "\n",
    "#### 内容\n",
    "\n",
    "JDA的工作是TCA的延伸，在TCA当中，假设了在$P(\\mathcal{Y}|X_S) \\approx (\\mathcal{Y}|X_T)$, 然而在现实当中，这样的条件概率发生的情况很少，需要考虑到这个差异。作者在此基础上首先在源域上训练了一个分类器，然后通过建立在目标域的伪标签（pseudo label），并通过不断迭代来减少条件概率的差异。源域数据$\\in R^{m\\times n_s}$，目标域数据$\\in R^{m\\times n_t}$，目标寻求变换$A^T \\in R^{k\\times m}$。其步骤如下：\n",
    "\n",
    "- 1 首先在RKHS中定义如下距离(Maximum Mean Discrepancy)，此步骤与TCA类似：\n",
    "$$\\operatorname{Dist}(\\mathrm{X}, \\mathrm{Y})=\\left\\|\\frac{1}{n_{1}} \\sum_{i=1}^{n_{1}} \\phi\\left(x_{i}\\right)-\\frac{1}{n_{2}} \\sum_{i=1}^{n_{2}} \\phi\\left(y_{i}\\right)\\right\\|_{\\mathcal{H}}$$\n",
    "将上式替换成源域与目标域之间的关系，可得：\n",
    "$$\\operatorname{Dist}\\left(X_{S}^{\\prime}, X_{T}^{\\prime}\\right)=\\left\\|\\frac{1}{n_{1}} \\sum_{i=1}^{n_{1}} \\phi\\left(x_{S_{i}}\\right)-\\frac{1}{n_{2}} \\sum_{i=1}^{n_{2}} \\phi\\left(x_{T_{i}}\\right)\\right\\|_{\\mathcal{H}}^{2}$$\n",
    "在JDA中，这个映射函数可为一个线性映射函数$A^T$，即：\n",
    "$$\\left\\|\\frac{1}{n_{s}} \\sum_{i=1}^{n_{s}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{x}_{i}-\\frac{1}{n_{t}} \\sum_{j=n_{s}+1}^{n_{s}+n_{t}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{x}_{j}\\right\\|^{2}=\\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{X} \\mathbf{M}_{0} \\mathbf{X}^{\\mathrm{T}} \\mathbf{A}\\right)$$\n",
    "\n",
    "- 2 条件概率自适应，在源域的数据上学习分类器（SVM,TCA等均可）后，应用于目标域的数据并且标上标签，现在在源域与目标域都有标签的情况下，对于标签的每一类$c \\in {1,\\cdots,C}$，其条件概率的具体为：\n",
    "$$\\left\\|\\frac{1}{n_{s}^{(c)}} \\sum_{\\mathbf{x}_{i} \\in \\mathcal{D}_{s}^{(c)}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{x}_{i}-\\frac{1}{n_{t}^{(c)}} \\sum_{\\mathbf{x}_{j} \\in \\mathcal{D}_{t}^{(c)}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{x}_{j}\\right\\|^{2}=\\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{X} \\mathbf{M}_{c} \\mathbf{X}^{\\mathrm{T}} \\mathbf{A}\\right)$$\n",
    "其中$\\mathcal{D}_{s}^{(c)}=\\left\\{\\mathbf{x}_{i} : \\mathbf{x}_{i} \\in \\mathcal{D}_{s} \\wedge y\\left(\\mathbf{x}_{i}\\right)=c\\right\\}$是一组标签为$c$的源域实例。$n_s^{(c)}$为在源域上c类实例的个数，其余notation与之类似。\n",
    "其中$\\mathbf{M}_c$为：\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/B8Iz9rzRcym9G9mZOb2UCjv-Tv4QzNGAY6SK0Z2kS3A.original.fullsize.png\" width=\"400\" hegiht=\"213\" align=center />\n",
    "\n",
    "- 3 有了以上每一类的误差值，优化问题便转化为：\n",
    "$$\\min _{\\mathbf{A}^{\\mathrm{T}} \\mathbf{X} \\mathbf{H} \\mathbf{X}^{\\mathrm{T}} \\mathbf{A}=\\mathbf{I}} \\sum_{c=0}^{C} \\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{X} \\mathbf{M}_{c} \\mathbf{X}^{\\mathrm{T}} \\mathbf{A}\\right)+\\lambda\\|\\mathbf{A}\\|_{F}^{2}$$\n",
    "这个问题包含了以下几个目标函数：\n",
    "    - 源域与目标域数据本身分布之间的差异。\n",
    "    - 对于标签的条件概率分布之间的差异。\n",
    "    - 映射后的数据要尽量保持大的方差。即结构保持不变，同TCA。\n",
    "   \n",
    "- 4 同时作者也给出了核-JDA的方法，即将$A^T$替换为$K$,既得：\n",
    "$$\\min _{\\mathbf{A}^{\\mathrm{T}} \\mathbf{K} \\mathbf{H}^{\\mathrm{T}} \\mathbf{A}=\\mathbf{I}} \\sum_{c=0}^{C} \\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{K} \\mathbf{M}_{c} \\mathbf{K}^{\\mathrm{T}} \\mathbf{A}\\right)+\\lambda\\|\\mathbf{A}\\|_{F}^{2}$$\n",
    "\n",
    "- 5 通过拉格朗日算子去解得以上的优化目标，可得：\n",
    "\\begin{aligned} L &=\\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}}\\left(\\mathbf{X} \\sum_{c=0}^{C} \\mathbf{M}_{c} \\mathbf{X}^{\\mathrm{T}}+\\lambda \\mathbf{I}\\right) \\mathbf{A}\\right) \\\\ &+\\operatorname{tr}\\left(\\left(\\mathbf{I}-\\mathbf{A}^{\\mathrm{T}} \\mathbf{X} \\mathbf{H} \\mathbf{X}^{\\mathrm{T}} \\mathbf{A}\\right) \\mathbf{\\Phi}\\right) \\end{aligned}\n",
    "通过对$A$求导，可得到一下的规范化的特征值分解：\n",
    "$$ \\left(\\mathbf{X} \\sum_{c=0}^{C} \\mathbf{M}_{c} \\mathbf{X}^{\\mathrm{T}}+\\lambda \\mathbf{I}\\right) \\mathbf{A}=\\mathbf{X} \\mathbf{H} \\mathbf{X}^{\\mathrm{T}} \\mathbf{A} \\boldsymbol{\\Phi} $$\n",
    "即以下的目标函数：\n",
    "$$\\left(\\mathbf{X} \\mathbf{H} \\mathbf{X}^{\\mathrm{T}} \\right)^{-1}\\left(\\mathbf{X} \\sum_{c=0}^{C} \\mathbf{M}_{c} \\mathbf{X}^{\\mathrm{T}}+\\lambda \\mathbf{I}\\right) \\mathbf{A}= \\lambda \\mathbf{A}  $$\n",
    "选择矩阵$\\left(\\mathbf{X} \\mathbf{H} \\mathbf{X}^{\\mathrm{T}} \\right)^{-1}\\left(\\mathbf{X} \\sum_{c=0}^{C} \\mathbf{M}_{c} \\mathbf{X}^{\\mathrm{T}}+\\lambda \\mathbf{I}\\right) \\in R^{n \\times n}$最小的$k$个特征值所构成的特征矩阵即为$A$。\n",
    "\n",
    "- 6 再每次求得$A$后，不断迭代源域标签直至收敛即可，算法如下：\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/N-H5_O1o1juU-gCdWDZyF-aUD_YUFCkkVk6I15dEn0Q.original.fullsize.png\" width=\"500\" hegiht=\"313\" align=center />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-08-15\n",
    "\n",
    "\n",
    "## Paper 1\n",
    "\n",
    "### A Comparative Study on Unsupervised Domain Adaptation Approaches for Coffee Crop Mapping\n",
    "\n",
    "IEEE Geoscience and Remote Sensing Letters, 2018\n",
    "\n",
    "Bibcitation: @article{ferreira2018comparative,\n",
    "  title={A Comparative Study on Unsupervised Domain Adaptation Approaches for Coffee Crop Mapping},\n",
    "  author={Ferreira, Edemir and Alvim, M{\\'a}rio S and Santos, Jefersson A dos},\n",
    "  journal={arXiv preprint arXiv:1806.02400},\n",
    "  year={2018}\n",
    "}\n",
    "\n",
    "轻松一下，迁移学习在咖啡作物的模式识别。此论文为迁移学习在农业当中的应用。 作者主要对比了现有的一些无监督学习的领域自适应方法，来识别不同的咖啡种类，因为labeling需要很大的cost blabla之类的。无监督领域自适应（UDA）的方法主要分为基于实例的，基于特征的与基于分类器的。本文主要考察的是基于特征的UDA方法。数据中心化的方法有TCA, JDA, TJM，子空间中心化的方法有 SA, GFK， 混合的方法有CORAL与JGSA。\n",
    "\n",
    "TCA是将在RKHS中的数据映射至子空间，最小化边缘概率并保持方差。\n",
    "JDA是最小化边缘概率差异的同时，同时缩小预测条件概率，从而在具有很大的域差异时仍保持高效的迁移。\n",
    "TJM在最小化边缘概率差异的同时，对分类具有更大帮助的实例进行加权，是基于特征与基于实例的结合。\n",
    "这三种方法的前提是源域与目标域一定具有一个映射使得两个数据集具有一定的相似性，然而在现实中，如果两个数据集完全不同，则可能找不到这样的映射。\n",
    "\n",
    "SA方法将源域与目标域通过PCA映射至不同的子空间，然后学习一个线性矩阵$M$，来将源域映射至目标域以缩小他们之间的距离（F范数）。\n",
    "GFK是测地流核方法，是一种非常优雅的方法，它通过核方法集成了在测地流的无数个源域子空间至目标域子空间。\n",
    "子空间中心化的方法的优点是保持了数据的几何结构，但是未加考虑数据的边缘概率。而且子空间具有超参调整，需要一定的计算量。\n",
    "\n",
    "混合类的方法有CORAL与JGSA，CORAL通过最大化源域与目标域之间（**分布的**）的方差，与子空间方法不同的是，CORAL特征对齐不需要额外的子空间映射，然而他需要大量的计算与较高的复杂度，而且它只寻求一个**非对称**的映射，即将源域映射至目标域。\n",
    "JGSA目标在于通过找到源域与目标域之间的共同特征与独有特征，从而减少统计与几何之间的域差异。为了实现这一目标，一个总体的目标函数被考虑，即目标域的方差，不同类别之间的方差，边缘概率差异，与子空间偏移。\n",
    "\n",
    "总结图如下：\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/ZRsJypleI1SsKy-Vqm8-88kxbmlpLn2g6Frvn6tyJgc.original.fullsize.png\" width=\"500\" hegiht=\"313\" />\n",
    "\n",
    "\n",
    "在咖啡作物的数据集上结果如下图所示\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/DVKhEjm6Qr-aaB8HNKo-Q8_4Cm8TPU0BC9Vlq5S7H_c.original.fullsize.png\" />\n",
    "从结果来看，UDA的方法并非都是正迁移，负迁移时常会发生。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-08-16\n",
    "\n",
    "\n",
    "## Paper 1\n",
    "\n",
    "\n",
    "### Return to Frustratingly Easy Domain Adaptation(CORAL)\n",
    "\n",
    "Conference: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)\n",
    "\n",
    "Bibcitation: @inproceedings{sun2016return,\n",
    "  title={Return of frustratingly easy domain adaptation},\n",
    "  author={Sun, Baochen and Feng, Jiashi and Saenko, Kate},\n",
    "  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},\n",
    "  year={2016}\n",
    "}\n",
    "\n",
    "#### 内容\n",
    "\n",
    "本文使用了一种非常简单的通过方差对齐来实现迁移学习的例子。美其名曰CORal ALignment(CORAL)，是一种领域自适应的非监督学习。文章主要考虑的是数据的二阶特性，通过方差去做白化（whitening）与重彩（re-color），首先是针对于源域数据进行白化，再通过目标域的方差进行数据的重彩，其实现细节如下所示：\n",
    "\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/PNuvpSk686t0dLsyugVFzv0auZFAgmSEzDci8-E_N7w.original.fullsize.png\" width=\"500\" hegiht=\"313\" align=center />\n",
    "\n",
    "图a代表的是具有不同分布的源域与数据域（数据已经归一化），图b代表的是去除了源域的特征相关性后的情况，图c是将源域的相关性加入至目标域的相关，也就是本文要实现的目的，这样数据会在同一个域里被适配，在源域中训练的模型会比较好的应用于目标域。图d是将源域与目标域都白化后的结果，然而这种方法可能不会work，因为源域和目标域因域偏移仍会落在不同的子空间中。$D_S \\in R^{n_s \\times N}，D_T \\in R^{n_T \\times N}$，CORAL的方法如下：\n",
    "\n",
    "- 1： 首先计算目标域与源域的协方差：\n",
    "$$C_S = (D_S - \\mu_S)^T \\times (D_S - \\mu_S) \\in R^{N \\times N}$$\n",
    "$$C_T = (D_T - \\mu_T)^T \\times (D_T - \\mu_T) \\in R^{N \\times N}$$\n",
    "其中$\\mu_S,\\mu_T$分别为源域与目标域的特征均值矢量。\n",
    "\n",
    "- 2： 寻求变换$A$,优化以下目标：\n",
    "\\begin{array}{l}{\\min _{A}\\left\\|C_{\\hat{S}}-C_{T}\\right\\|_{F}^{2}} \\\\ {=\\min _{A}\\left\\|A^{\\top} C_{S} A-C_{T}\\right\\|_{F}^{2}}\\end{array}\n",
    "求解以上的优化问题需要用到以下引理：\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/g5FjKqoZr0224N-towVSZ4LFSZJzXjxwcZUaxbjNi3s.original.fullsize.png\" width=\"500\" hegiht=\"313\" align=center />\n",
    "即在限定$X$的秩的情况下，能够距离$Y$误差最小的矩阵是其SVD分解后前$r$(秩)个解/矢量的组合。作者通过这个引理，推导出以下的定理：\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/ppkTHnagMFk5yoCq_KlcOL7Ct1F9anjlRtV0XsbRabs.original.fullsize.png\" width=\"500\" hegiht=\"313\" align=center/>\n",
    "推导分为两个步骤，第一，若源域的秩大于目标域的秩，则必能找到一个线性变换，使得$C_{\\hat{S}} = C_T$，所以：\n",
    "$$\n",
    "C_{\\hat{S}}=U_{T} \\Sigma_{T} U_{T}^{\\top}=U_{T[1 : r]} \\Sigma_{T[1 : r]} U_{T[1 : r]}^{T}\n",
    "$$\n",
    "第二种情况，若源域的秩小于等于目标域的秩，通过引理，可得：\n",
    "$$\n",
    "C_{\\hat{S}}= U_{T[1 : r]} \\Sigma_{T[1 : r]} U_{T[1 : r]}^{T}\n",
    "$$\n",
    "所以均可以得到这样的解：\n",
    "$$\n",
    "C_{\\hat{S}}= U_{T[1 : r]} \\Sigma_{T[1 : r]} U_{T[1 : r]}^{T}\n",
    "$$\n",
    "我们的目的是寻找$A$，所以：\n",
    "$$\n",
    "A^{\\top} C_{S} A=U_{T[1 : r]} \\Sigma_{T[1 : r]} U_{T[1 : r]}^{\\top}\n",
    "$$\n",
    "由奇异值分解，$C_S = U_S\\Sigma_SU_S^T$，有：\n",
    "$$\n",
    "A^{\\top} U_{S} \\Sigma_{S} U_{S}^{\\top} A=U_{T[1 : r]} \\Sigma_{T[1 : r]} U_{T[1 : r]}^{\\top}\n",
    "$$\n",
    "即：\n",
    "$$\n",
    "\\left(U_{S}^{\\top} A\\right)^{\\top} \\Sigma_{S}\\left(U_{S}^{\\top} A\\right)=U_{T[1 : r]} \\Sigma_{T[1 : r]} U_{T[1 : r]}^{\\top}\n",
    "$$\n",
    "**构造矩阵$E$**，这里作者的做法是构造白化来构造E：\n",
    "$$\n",
    "E=\\Sigma_{S}^{+\\frac{1}{2}} U_{S}^{\\top} U_{T[1 : r]} \\Sigma_{T[1 : r]}^{\\frac{1}{2}} U_{T[1 : r]}^{\\top}\n",
    "$$\n",
    "从而：\n",
    "$$\n",
    "\\left(U_{S}^{\\top} A\\right)^{\\top} \\Sigma_{S}\\left(U_{S}^{\\top} A\\right)=E^{\\top} \\Sigma_{S} E\n",
    "$$\n",
    "所以可得最优的解$A$:\n",
    "\\begin{aligned} A^{*} &=U_{S} E \\\\ &=\\left(U_{S} \\Sigma_{S}^{+\\frac{1}{2}} U_{S}^{\\top}\\right)\\left(U_{T[1 : r]} \\Sigma_{T[1 : r]}^{\\frac{1}{2}} U_{T[1 : r]}^{\\top}\\right) \\end{aligned}\n",
    "\n",
    "- 3 从以上的式子来看，由于数据变换是通过$D_SA$来实现的，$A$中的第一项为通过原始数据进行白化，第二项则为通过目标域的方差对源域数据进行重彩，是一种非对称的变换，然而这种方法只利用了方差进行变换，非常简单，而且算法在人脸识别与物体识别中取得了非常好的效果。算法如下：\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/DExAgHxEBkpp93yiZxvx9m7Eh7g9fslt8_0hgD4XfyE.original.fullsize.png\" width=\"500\" hegiht=\"313\" align=center/>\n",
    "\n",
    "\n",
    "## Paper 2\n",
    "\n",
    "### Transfer Joint Matching for Unsupervised Domain Adaptation\n",
    "\n",
    "Conference: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR-2014)\n",
    "\n",
    "Bibcitation: @inproceedings{long2014transfer,\n",
    "  title={Transfer joint matching for unsupervised domain adaptation},\n",
    "  author={Long, Mingsheng and Wang, Jianmin and Ding, Guiguang and Sun, Jiaguang and Yu, Philip S},\n",
    "  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n",
    "  pages={1410--1417},\n",
    "  year={2014}\n",
    "}\n",
    "\n",
    "\n",
    "#### 内容\n",
    "\n",
    "这是龙老师2014年CVPR的论文，仍然是基于原有TCA工作进行改进的一些方法，本文的思想是在TCA的基础上引入了L2,1范数，在映射矩阵$A$中，最小化其L2,1的作用为每一行范数之和最小才能够使得整体范数最小，即这样的解是**稀疏的**，即解当中的每一行定会存在很小或趋向于0的数，因为映射变换为$A^T\\mathcal{D}_S$，这就相当于给$\\mathcal{D}_S$的每一列进行了不同的权重分配，即在源域中有一些实例的权重会大一些，而有一些实例的权重会小一些。所以TJM即最小化了MMD，又将instance weighting结合了起来，是非常漂亮的工作。其算法的实现步骤如下：\n",
    "\n",
    "- 1：PCA是针对散度（方差）进行降维的，作者首先提出了核函数的PCA，即Kernel-PCA，存在一个映射$V=\\phi(X)A$，满足以下的优化条件：\n",
    "$$\n",
    "\\max _{\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}=\\mathbf{I}} \\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{K} \\mathbf{H} \\mathbf{K}^{\\mathrm{T}} \\mathbf{A}\\right)\n",
    "$$\n",
    "其中$A \\in R^{n\\times k}$为降维矩阵。那么子空间嵌入则为：$Z = V^T\\times \\phi(X) = A^T\\times K$。\n",
    "\n",
    "- 2：建立核函数的PCA来优化MMD：\n",
    "$$\n",
    "\\left\\|\\frac{1}{n_{s}} \\sum_{i=1}^{n_{s}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{k}_{i}-\\frac{1}{n_{t}} \\sum_{j=n_{s}+1}^{n_{s}+n_{t}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{k}_{j}\\right\\|_{\\mathcal{H}}^{2}=\\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{K} \\mathbf{M} \\mathbf{K}^{\\mathrm{T}} \\mathbf{A}\\right)\n",
    "$$\n",
    "其中$M$矩阵为MMD矩阵，即TCA当中的L矩阵，即：\n",
    "$$\n",
    "M_{i j}=\\left\\{\\begin{array}{ll}{\\frac{1}{n_{s} n_{s}},} & {\\mathbf{x}_{i}, \\mathbf{x}_{j} \\in \\mathcal{D}_{s}} \\\\ {\\frac{1}{n_{t} n_{t}},} & {\\mathbf{x}_{i}, \\mathbf{x}_{j} \\in \\mathcal{D}_{t}} \\\\ {\\frac{-1}{n_{s} n_{t}},} & {\\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "- 3： 实例权重，作者通过引入$L_{2,1}$范数对源域数据实例化，其优化的目标则变成了：\n",
    "$$\n",
    "\\min _{\\mathbf{A}^{\\mathrm{T}} \\mathbf{K} \\mathbf{H} \\mathbf{K}^{\\mathrm{T}} \\mathbf{A}=\\mathbf{I}} \\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{K} \\mathbf{M} \\mathbf{K}^{\\mathrm{T}} \\mathbf{A}\\right)+\\lambda\\left(\\left\\|\\mathbf{A}_{s}\\right\\|_{2,1}+\\left\\|\\mathbf{A}_{t}\\right\\|_{F}^{2}\\right)\n",
    "$$\n",
    "\n",
    "- 4： 算法实现，通过写出拉格朗日优化函数，可得：\n",
    "\\begin{aligned} L &=\\operatorname{tr}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{K} \\mathbf{M} \\mathbf{K}^{\\mathrm{T}} \\mathbf{A}\\right)+\\lambda\\left(\\left\\|\\mathbf{A}_{s}\\right\\|_{2,1}+\\left\\|\\mathbf{A}_{t}\\right\\|_{F}^{2}\\right) \\\\ &+\\operatorname{tr}\\left(\\left(\\mathbf{I}-\\mathbf{A}^{\\mathrm{T}} \\mathbf{K} \\mathbf{H} \\mathbf{K}^{\\mathrm{T}} \\mathbf{A}\\right) \\mathbf{\\Phi}\\right) \\end{aligned}\n",
    "将$\\frac{\\partial L}{\\partial A}=0$，则$A$的解如下：\n",
    "$$\\left(\\mathbf{K M K}^{\\mathrm{T}}+\\lambda \\mathbf{G}\\right) \\mathbf{A}=\\mathbf{K H K}^{\\mathrm{T}} \\mathbf{A} \\Phi$$\n",
    "其中$G$为：\n",
    "$$G_{i i}=\\left\\{\\begin{array}{ll}{\\frac{1}{2\\left\\|\\mathbf{a}^{i}\\right\\|},} & {\\mathbf{x}_{i} \\in \\mathcal{D}_{s}, \\mathbf{a}^{i} \\neq \\mathbf{0}} \\\\ {0,} & {\\mathbf{x}_{i} \\in \\mathcal{D}_{s}, \\mathbf{a}^{i}=0} \\\\ {1,} & {\\mathbf{x}_{i} \\in \\mathcal{D}_{t}}\\end{array}\\right.$$\n",
    "\n",
    "- 5： G与A可通过不断迭代得到。在原论文中作者给出了收敛性的分析，有兴趣可查看原文。算法总体如下：\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/j6JZwVEZxbaMQWrdriYh6x7o2bi-35hyvOWRBGQ-Ufw.original.fullsize.png\" width=\"500\" hegiht=\"313\" align=center/>\n",
    "\n",
    "作者在手写数字的数据集以及亚马逊OFFICE之类的数据集上进行了测试，并且对超参进行分析，也对于源域实例的权重对TCA进行了相应的对比，工作严谨，效果出众，令人折服。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-08-18\n",
    "\n",
    "\n",
    "## Paper 1\n",
    "\n",
    "### Unsupervised Visual Domain Adaptation Using Subspace Alignment\n",
    "\n",
    "Conference: Proceedings of the IEEE international conference on computer vision 2013 (ICCV-13)\n",
    "\n",
    "Bibcitation:@inproceedings{fernando2013unsupervised,\n",
    "  title={Unsupervised visual domain adaptation using subspace alignment},\n",
    "  author={Fernando, Basura and Habrard, Amaury and Sebban, Marc and Tuytelaars, Tinne},\n",
    "  booktitle={Proceedings of the IEEE international conference on computer vision},\n",
    "  pages={2960--2967},\n",
    "  year={2013}\n",
    "}\n",
    "\n",
    "#### 内容\n",
    "\n",
    "\n",
    "## Paper 2\n",
    "\n",
    "### Joint geometrical and statistical alignment for visual domain adaptation\n",
    "\n",
    "Conference: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR-17)\n",
    "\n",
    "Bibcitation: @inproceedings{zhang2017joint,\n",
    "  title={Joint geometrical and statistical alignment for visual domain adaptation},\n",
    "  author={Zhang, Jing and Li, Wanqing and Ogunbona, Philip},\n",
    "  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n",
    "  pages={1859--1867},\n",
    "  year={2017}\n",
    "}\n",
    "\n",
    "#### 内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Basic Infomation\n",
    "\n",
    "\n",
    "## Laplacian Matrix(拉普拉斯矩阵)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
